{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Assignment: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Building an MLP on the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MNIST training examples: 60000\n",
      "Number of MNIST test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#download train and test datasets from MNIST and transform to tensors\n",
    "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "print(\"Number of MNIST training examples: {}\".format(len(mnist_train)))\n",
    "print(\"Number of MNIST test examples: {}\".format(len(mnist_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default image shape: torch.Size([1, 28, 28])\n",
      "Reshaped image shape: torch.Size([28, 28])\n",
      "The label for this image: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick out the 9th (0-indexed) example from the training set\n",
    "image, label = mnist_train[10001]\n",
    "\n",
    "# Plot the image\n",
    "print(\"Default image shape: {}\".format(image.shape))\n",
    "image = image.reshape([28,28])\n",
    "print(\"Reshaped image shape: {}\".format(image.shape))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "# Print the label\n",
    "print(\"The label for this image: {}\".format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader takes care of batching and shuffling\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size = 200, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size = 200, shuffle = False)\n",
    "#create an iterator object\n",
    "data_train_iter = iter(train_loader)\n",
    "images, labels = data_train_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of input x_2: torch.Size([200, 784])\n"
     ]
    }
   ],
   "source": [
    "#view() reshapes the tensor (tensor_name.view())\n",
    "#Ex: a = torch.range(1, 16)\n",
    "#a = a.view(4, 4) -> changes the above tensor 'a', from a 1x16 to a 4x4 tensor\n",
    "#-1 parameter means that if you do not know how many rows you want, but are sure of the number of columns, you can \n",
    "# use -1 -> give me a tensor that has this many columns (28x28 below) and you compute the number of rows to make this \n",
    "#happen\n",
    "x_2 = images.view(-1, 28*28)\n",
    "print(\"The shape of input x_2: {}\".format(x_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalializing a 500-d hidden layer. W_500 is randomized and b starts at zero\n",
    "W = torch.randn(784, 500)/np.sqrt(784)\n",
    "W.requires_grad_()\n",
    "#print(W_500)\n",
    "b = torch.zeros(500, requires_grad=True)\n",
    "#print(b_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4060,  0.3533,  0.1263,  ..., -0.4602, -0.0892,  0.2790],\n",
      "        [-0.3982,  0.1660,  0.1402,  ...,  0.0668,  0.0069, -0.0252],\n",
      "        [-0.0941,  0.3177,  0.3979,  ..., -0.2908, -0.3049, -0.4494],\n",
      "        ...,\n",
      "        [-0.2490,  0.4897,  0.3237,  ..., -0.0070,  0.1024,  0.2005],\n",
      "        [ 0.0413,  0.2355, -0.0094,  ..., -0.5556,  0.2227,  0.2373],\n",
      "        [-0.0826, -0.3315,  0.7326,  ...,  0.1103, -0.1773,  0.0295]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Perform a linear transform with W and b\n",
    "y = torch.matmul(x_2, W) + b\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 500])\n",
      "x after ReLU with nn.functional: tensor([[0.0000, 0.3533, 0.1263,  ..., 0.0000, 0.0000, 0.2790],\n",
      "        [0.0000, 0.1660, 0.1402,  ..., 0.0668, 0.0069, 0.0000],\n",
      "        [0.0000, 0.3177, 0.3979,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.4897, 0.3237,  ..., 0.0000, 0.1024, 0.2005],\n",
      "        [0.0413, 0.2355, 0.0000,  ..., 0.0000, 0.2227, 0.2373],\n",
      "        [0.0000, 0.0000, 0.7326,  ..., 0.1103, 0.0000, 0.0295]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Including a nonlinearity via the rectified linear unit\n",
    "y_relu_F = F.relu(y)\n",
    "print(format(y_relu_F.shape))\n",
    "\n",
    "print(\"x after ReLU with nn.functional: {}\".format(y_relu_F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalializing a 10-d hidden layer. W is randomized and b starts at zero\n",
    "#optimizer.zero_grad()\n",
    "W_500 = torch.randn(500, 10)/np.sqrt(500)\n",
    "W_500.requires_grad_()\n",
    "#print(W)\n",
    "b_500 = torch.zeros(10, requires_grad=True)\n",
    "#print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression #2\n",
    "y = torch.matmul(y_relu_F, W_500) + b_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy with torch.nn.functional.cross_entropy: 2.3494749069213867\n"
     ]
    }
   ],
   "source": [
    "#Performing cross entropy, softmax is included within this function in PyTorch\n",
    "cross_entropy = F.cross_entropy(y, labels)\n",
    "print(\"cross entropy with torch.nn.functional.cross_entropy: {}\".format(cross_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the optimizer function, this will be Stoichastic Gradient Descent\n",
    "optimizer = torch.optim.SGD([W,b,W_500,b_500], lr=0.1)\n",
    "#print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calling the backward function on the cross_entropy above to compute the gradients for W and b\n",
    "cross_entropy.backward()\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the gradient step\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear gradient buffers as gradients calculated by backward() accumulate\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f72ae2578cd4ee79fad843c174fa2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Now that I have the completed steps and they all look good, I am adding the full MLP code below. \n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "#Initialize all variables\n",
    "#First \n",
    "W = torch.randn(784, 500)/np.sqrt(784)\n",
    "W.requires_grad_()\n",
    "#print(W)\n",
    "b = torch.zeros(500, requires_grad=True)\n",
    "#print(b)\n",
    "#Second\n",
    "W2 = torch.randn(500, 10)/np.sqrt(500)\n",
    "W2.requires_grad_()\n",
    "#print(W2)\n",
    "b2 = torch.zeros(10, requires_grad=True)\n",
    "#print(b2)\n",
    "\n",
    "optimizer = torch.optim.SGD([W,b,W2,b2], lr=0.05)\n",
    "\n",
    "# Iterate through train set minibatchs \n",
    "for images, labels in tqdm(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    x2 = images.view(-1, 28*28)\n",
    "    y = torch.matmul(x2, W) + b\n",
    "    y_relu = F.relu(y)\n",
    "    y2_lay = torch.matmul(y_relu, W2) + b2\n",
    "    cross_entropy = F.cross_entropy(y2_lay, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    cross_entropy.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88208b950de74781a54d589f1aba5573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9117000102996826\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = len(mnist_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs \n",
    "    for images, labels in tqdm(test_loader):\n",
    "        # Forward pass\n",
    "        x2 = images.view(-1, 28*28)\n",
    "        y = torch.matmul(x2, W) + b\n",
    "        y_relu = F.relu(y)\n",
    "        y2_lay = torch.matmul(y_relu, W2) + b2\n",
    "        cross_entropy = F.cross_entropy(y2_lay, labels)\n",
    "        \n",
    "        \n",
    "        predictions = torch.argmax(y2_lay, dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "    \n",
    "print('Test accuracy: {}'.format(correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
